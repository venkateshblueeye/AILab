{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a. BSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def bfs(graph, start):\n",
    "    visited = set()  # Use a set for faster membership check\n",
    "    queue = deque([start])  # Use deque for efficient FIFO operations\n",
    "\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        if node not in visited:\n",
    "            print(node, end=\" \")  # Print the node being visited\n",
    "            visited.add(node)\n",
    "            queue.extend(graph[node])\n",
    "\n",
    "# Define the graph\n",
    "graph = {\n",
    "    '5': ['3', '7'],\n",
    "    '3': ['2', '4'],\n",
    "    '7': ['8'],\n",
    "    '2': [],\n",
    "    '4': ['8'],\n",
    "    '8': []\n",
    "}\n",
    "\n",
    "# Driver code\n",
    "print(\"Following is the Breadth-First Search\")\n",
    "bfs(graph, '5')  # Start BFS from node '5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b. DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs(graph, start, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    visited.add(start)\n",
    "    print(start, end=' ')\n",
    "\n",
    "    for neighbor in graph[start]:\n",
    "        if neighbor not in visited:\n",
    "            dfs(graph, neighbor, visited)\n",
    "\n",
    "# Example graph\n",
    "graph = {\n",
    "    '5': ['3', '7'],\n",
    "    '3': ['2', '4'],\n",
    "    '7': ['8'],\n",
    "    '2': [],\n",
    "    '4': ['8'],\n",
    "    '8': []\n",
    "}\n",
    "\n",
    "# Driver code\n",
    "print(\"Following is the Depth-First Search\")\n",
    "dfs(graph, '5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A* Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def heuristic(node, goal):\n",
    "    # Define your heuristic function here.\n",
    "    # This function should estimate the cost from the current node to the goal node.\n",
    "    # Common heuristics include Manhattan distance, Euclidean distance, etc.\n",
    "    # In this example, let's assume no heuristic (heuristic value always 0).\n",
    "    return 0\n",
    "\n",
    "def astar(graph, start, goal):\n",
    "    # Open list to store nodes to be evaluated\n",
    "    open_list = [(0, start)]  # Tuple (f_cost, node)\n",
    "    # Closed list to store visited nodes\n",
    "    closed_list = set()\n",
    "\n",
    "    # Dictionary to store the cost from the start node to each node\n",
    "    g_cost = {node: float('inf') for node in graph}\n",
    "    g_cost[start] = 0\n",
    "\n",
    "    # Dictionary to store the parent node of each node\n",
    "    parents = {}\n",
    "\n",
    "    while open_list:\n",
    "        # Pop node with the minimum f_cost from open_list\n",
    "        f_cost, current_node = heapq.heappop(open_list)\n",
    "\n",
    "        # Check if current node is the goal\n",
    "        if current_node == goal:\n",
    "            path = []\n",
    "            while current_node in parents:\n",
    "                path.append(current_node)\n",
    "                current_node = parents[current_node]\n",
    "            path.append(start)\n",
    "            return path[::-1]  # Reverse the path to start -> goal\n",
    "         \n",
    "        closed_list.add(current_node)\n",
    "\n",
    "        # Explore neighbors of the current node\n",
    "        for neighbor in graph[current_node]:\n",
    "            # Calculate tentative g_cost from start to neighbor through current_node\n",
    "            tentative_g_cost = g_cost[current_node] + graph[current_node][neighbor]\n",
    "\n",
    "            # Update if this path is better than the previous one\n",
    "            if tentative_g_cost < g_cost[neighbor]:\n",
    "                g_cost[neighbor] = tentative_g_cost\n",
    "                f_cost = tentative_g_cost + heuristic(neighbor, goal)\n",
    "                heapq.heappush(open_list, (f_cost, neighbor))\n",
    "                parents[neighbor] = current_node\n",
    "\n",
    "    # If no path found\n",
    "    return None\n",
    "\n",
    "# Example graph\n",
    "graph = {\n",
    "    'A': {'B': 1, 'C': 4},\n",
    "    'B': {'A': 1, 'C': 2, 'D': 5},\n",
    "    'C': {'A': 4, 'B': 2, 'D': 1},\n",
    "    'D': {'B': 5, 'C': 1}\n",
    "}\n",
    "\n",
    "# Driver code\n",
    "start_node = 'A'\n",
    "goal_node = 'D'\n",
    "path = astar(graph, start_node, goal_node)\n",
    "\n",
    "if path:\n",
    "    print(\"Path found:\", ' -> '.join(path))\n",
    "else:\n",
    "    print(\"No path found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6488402154431997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Drop the rows with missing values\n",
    "url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "# Convert the categorical variable to numerical using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['ocean_proximity'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('median_house_value', axis=1), df['median_house_value'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the online dataset\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header=None, names=column_names)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define categorical features for one-hot encoding\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Perform one-hot encoding\n",
    "ct = ColumnTransformer([('onehot', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "X_train_encoded = ct.fit_transform(X_train)\n",
    "X_test_encoded = ct.transform(X_test)\n",
    "\n",
    "# Build a decision tree classifier\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Build a random forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "dt_accuracy = accuracy_score(y_test, dt_model.predict(X_test_encoded))\n",
    "rf_accuracy = accuracy_score(y_test, rf_model.predict(X_test_encoded))\n",
    "\n",
    "print('Decision Tree Accuracy:', dt_accuracy)\n",
    "print('Random Forest Accuracy:', rf_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Build an SVM model using online dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = df.iloc[:, 2:].values\n",
    "y = df.iloc[:, 1].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM model\n",
    "clf = SVC()\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "score = clf.score(X_test, y_test)\n",
    "print('Accuracy:', score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Implementation of Ensembling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the classifiers\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train and evaluate each classifier individually\n",
    "for clf, name in zip([rf_clf, ada_clf, gb_clf], ['Random Forest', 'AdaBoost', 'Gradient Boosting']):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{name} Accuracy:', accuracy)\n",
    "\n",
    "# Ensemble classifiers\n",
    "ensemble_clf = VotingClassifier(estimators=[('rf', rf_clf), ('ada', ada_clf), ('gb', gb_clf)], voting='soft')\n",
    "\n",
    "# Train ensemble classifier\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the ensemble classifier\n",
    "y_pred = ensemble_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of ensemble classifier\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Ensemble Accuracy:', ensemble_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Various Clustering algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset from an online source\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of classification algorithms\n",
    "algorithms = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    GaussianNB(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "]\n",
    "\n",
    "# Train and evaluate each algorithm\n",
    "for algorithm in algorithms:\n",
    "    model = algorithm.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{algorithm.__class__.__name__} accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Simple NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from the UCI Machine Learning Repository\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "column_names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Convert class labels to numeric values\n",
    "encoder = LabelEncoder()\n",
    "dataset['class'] = encoder.fit_transform(dataset['class'])\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = dataset.drop('class', axis=1)\n",
    "y = dataset['class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. DeepLeaning NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),  # Dropout layer for regularization\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout layer for regularization\n",
    "    Dense(3, activation='softmax')  # Output layer with softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',  # Sparse categorical crossentropy for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=-1)\n",
    "print('Test Accuracy (using sklearn):', accuracy_score(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 .Implementation of  Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement Bayesian Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv \n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "heartDisease = pd.read_csv('E:/Subjects/BE/CS3491_AI/7-dataset.csv')\n",
    "heartDisease = heartDisease.replace('?',np.nan)\n",
    "\n",
    "print('Sample instances from the dataset are given below')\n",
    "print(heartDisease.head())\n",
    "\n",
    "print('\\n Attributes and datatypes')\n",
    "print(heartDisease.dtypes)\n",
    "\n",
    "model= BayesianModel([('age','heartdisease'),('gender','heartdisease'),('exang','heartdisease'),('cp','heartdisease'),('heartdisease','restecg'),('heartdisease','chol')])\n",
    "print('\\nLearning CPD using Maximum likelihood estimators')\n",
    "model.fit(heartDisease,estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "print('\\n Inferencing with Bayesian Network:')\n",
    "HeartDiseasetest_infer = VariableElimination(model)\n",
    "\n",
    "print('\\n 1. Probability of HeartDisease given evidence= restecg')\n",
    "q1=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'restecg':1})\n",
    "print(q1)\n",
    "\n",
    "print('\\n 2. Probability of HeartDisease given evidence= cp ')\n",
    "q2=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'cp':2})\n",
    "print(q2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
